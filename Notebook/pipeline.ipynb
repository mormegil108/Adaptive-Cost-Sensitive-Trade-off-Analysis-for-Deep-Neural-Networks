{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 554 - Introduction to Machine Learning and Artificial Neural Networks\n",
    "### Adaptive Cost-Sensitive Trade-off Analysis for Deep Neural Networks\n",
    "**Authors:** A. E., Bolluk S., T. E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDMGQtxxHjfw"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3_MVFv6Hjf5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import shutil\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F \n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "from prefetch_generator import BackgroundGenerator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.backends.cudnn.version()\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "np.random.seed(42)\n",
    " \n",
    "torch.manual_seed(42)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(42)\n",
    "    print(f\"Using cuda: {torch.cuda.get_device_name()} with capability {torch.cuda.get_device_capability()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7zDnIDWHjf8"
   },
   "source": [
    "## Experiment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mR3SgXhVHjf-"
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "class ExperimentModel(enum.Enum):\n",
    "   MLP      = 1\n",
    "   VGG      = 2\n",
    "   RESNET   = 3\n",
    "\n",
    "class ExperimentDataset(enum.Enum):\n",
    "   CIFAR10        = 1\n",
    "   FMNIST         = 2\n",
    "   INTELIMAGE     = 3\n",
    "   FMNIST_IMB70   = 4\n",
    "   FMNIST_IMB90   = 5 \n",
    "   CIFAR10_IMB90  = 7 \n",
    "   INTELIMAGE_IMB90  = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1-6YNHHHjf_"
   },
   "outputs": [],
   "source": [
    "experiment_model = ExperimentModel.RESNET\n",
    "experiment_dataset = ExperimentDataset.INTELIMAGE_IMB90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwFdrdK_HjgA"
   },
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptskxPQvHjgC",
    "outputId": "bd07405e-d2e7-4cd4-9c0c-0042ea8eb989"
   },
   "outputs": [],
   "source": [
    "current_datetime = datetime.datetime.now()\n",
    "timestamp = current_datetime.strftime(\"%y%m%d-%H%M%S\")\n",
    "\n",
    "if experiment_model == ExperimentModel.MLP:\n",
    "    experiment_name = f\"{timestamp}_mlp\"\n",
    "elif experiment_model == ExperimentModel.VGG:\n",
    "    experiment_name = f\"{timestamp}_vgg16\"\n",
    "elif experiment_model == ExperimentModel.RESNET:\n",
    "    experiment_name = f\"{timestamp}_resnet18\"\n",
    "else:\n",
    "    raise Exception('Unknown experimentation type')\n",
    "\n",
    "experiment_folder_path = os.path.join(f\"{os.getcwd()}\", \"experiment\")\n",
    "if not os.path.exists(experiment_folder_path):\n",
    "    #Create a new directory if not already exists\n",
    "    os.makedirs(experiment_folder_path)\n",
    "\n",
    "# Create a folder to store the results of all the experiments\n",
    "experiment_path = os.path.join(experiment_folder_path, experiment_name)\n",
    "\n",
    "if experiment_dataset == ExperimentDataset.CIFAR10 or experiment_dataset == ExperimentDataset.CIFAR10_IMB90:\n",
    "    class_json = \"classes_cifar.json\"\n",
    "elif experiment_dataset == ExperimentDataset.FMNIST or experiment_dataset == ExperimentDataset.FMNIST_IMB70 or experiment_dataset == ExperimentDataset.FMNIST_IMB90:\n",
    "    class_json = \"classes_fmnist.json\"\n",
    "\n",
    "\n",
    "elif experiment_dataset == ExperimentDataset.INTELIMAGE or experiment_dataset == ExperimentDataset.INTELIMAGE_IMB90:\n",
    "    class_json = \"classes_intelimage.json\"\n",
    "else:\n",
    "    raise Exception('Unknown dataset type')\n",
    "\n",
    "# Set Data Root\n",
    "data_root = os.path.join(f\"{os.getcwd()}\", \"data\") \n",
    "\n",
    "# Import Classes\n",
    "classes_path = os.path.join(data_root, class_json)\n",
    "with open(classes_path, \"r\") as json_file:\n",
    "    classes = json.load(json_file)\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy40_rh0HjgF"
   },
   "source": [
    "https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "https://www.kaggle.com/datasets/zalando-research/fashionmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMCmDoVsHjgG"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTGVvkl7HjgH"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module, only_trainable_parameters: bool = False):\n",
    "    \"\"\" Count the number of parameters in a model\n",
    "    Args:\n",
    "        model (nn.Module)\n",
    "        only_trainable_parameters (bool:False): only count the trainable parameters\n",
    "    Returns:\n",
    "        num_parameters (int): number of parameters in the model\n",
    "    \"\"\"\n",
    "    if only_trainable_parameters:\n",
    "        num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        num_parameters = sum(p.numel() for p in model.parameters())\n",
    "    return num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ep2WfVKsHjgI"
   },
   "outputs": [],
   "source": [
    "class MetricTracker:\n",
    "    \"\"\"Computes and stores the average and current value of a metric.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset all the tracked parameters \"\"\"\n",
    "        self.value = 0\n",
    "        self.average = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, value: float, num: int = 1):\n",
    "        \"\"\" Update the tracked parameters\n",
    "        Args:\n",
    "            value (float): new value to update the tracker with\n",
    "            num (int: 1): number of elements used to compute the value\n",
    "        \"\"\"\n",
    "        self.value = value\n",
    "        self.sum += value\n",
    "        self.count += num\n",
    "        self.average = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9Fx5P6OHjgJ"
   },
   "outputs": [],
   "source": [
    "class ConfusionMatrix:\n",
    "  \"\"\"Store, update and plot a confusion matrix.\"\"\"\n",
    "\n",
    "  def __init__(self, classes: dict):\n",
    "      \"\"\" Create and initialize a confusion matrix\n",
    "      Args:\n",
    "          classes (dict): dictionary containing all the classes (e.g. {\"0\": \"label_0\", \"1\": \"label_1\",...})\n",
    "      \"\"\"\n",
    "      self.classes = classes\n",
    "      self.num_classes = len(self.classes)\n",
    "      self.labels_classes = range(self.num_classes)\n",
    "      self.list_classes = list(self.classes.values())\n",
    "\n",
    "      self.cm = np.zeros([len(classes), len(classes)], dtype=int)\n",
    "\n",
    "  def update_confusion_matrix(self, targets: torch.Tensor, predictions: torch.Tensor):\n",
    "      \"\"\" Update the confusion matrix\n",
    "      Args:\n",
    "          targets (torch.Tensor): tensor on the cpu containing the target classes\n",
    "          predictions(torch.Tensor): tensor on the cpu containing the predicted classes\n",
    "      \"\"\"\n",
    "      # use sklearn to update the confusion matrix\n",
    "      self.cm += confusion_matrix(targets, predictions, labels=self.labels_classes)\n",
    "\n",
    "  def plot_confusion_matrix(\n",
    "      self,\n",
    "      normalize: bool = True,\n",
    "      title: str = None,\n",
    "      cmap: matplotlib.colors.Colormap = plt.cm.Blues,\n",
    "  ) -> matplotlib.figure.Figure:\n",
    "      \"\"\"\n",
    "      This function plots the confusion matrix.\n",
    "      Args:\n",
    "          normalize (bool: True): boolean to control the normalization of the confusion matrix.\n",
    "          title (str: \"\"): title for the figure\n",
    "          cmap (matplotlib.colors.Colormap: plt.cm.Blues): color map, defaults to 'Blues'\n",
    "      Returns:\n",
    "          matplotlib.figure.Figure: the ready-to-show/save figure\n",
    "      \"\"\"\n",
    "      if not title:\n",
    "          title = f\"Normalized Confusion Matrix\" if normalize else f\"Confusion Matrix\"\n",
    "\n",
    "       if normalize:\n",
    "          self.cm = self.cm.astype(\"float\") / np.maximum(\n",
    "              self.cm.sum(axis=1, keepdims=True), 1\n",
    "          )\n",
    "\n",
    "      # Create figure with size determined by number of classes.\n",
    "      fig, ax = plt.subplots(\n",
    "          figsize=[0.4 * self.num_classes + 4, 0.4 * self.num_classes + 2]\n",
    "      )\n",
    "      im = ax.imshow(self.cm, interpolation=\"nearest\", cmap=cmap)\n",
    "      ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "      # Show all ticks and label them with the respective list entries.\n",
    "      # Add a tick at the start and end in order to not cut off the figure.\n",
    "      ax.set(\n",
    "          xticklabels=[\"\"] + self.list_classes,\n",
    "          yticklabels=[\"\"] + self.list_classes,\n",
    "          xticks=np.arange(-1, self.cm.shape[1] + 1),\n",
    "          yticks=np.arange(-1, self.cm.shape[0] + 1),\n",
    "          \n",
    "          title=title,\n",
    "          ylabel=\"True label\",\n",
    "          xlabel=\"Predicted label\",\n",
    "      )\n",
    "\n",
    "      # Rotate the tick labels and set their alignment.\n",
    "      plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "      # Loop over data dimensions and create text annotations.\n",
    "      fmt = \".2f\" if normalize else \"d\"\n",
    "      thresh = self.cm.max() / 2.0\n",
    "      for i in range(self.cm.shape[0]):\n",
    "          for j in range(self.cm.shape[1]):\n",
    "              ax.text(\n",
    "                  j,\n",
    "                  i,\n",
    "                  format(self.cm[i, j], fmt),\n",
    "                  ha=\"center\",\n",
    "                  va=\"center\",\n",
    "                  color=\"white\" if self.cm[i, j] > thresh else \"black\",\n",
    "              )\n",
    "      fig.tight_layout()\n",
    "\n",
    "      return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Dbf5irwHjgK"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3TtIy5bHjgL"
   },
   "outputs": [],
   "source": [
    "if experiment_dataset == ExperimentDataset.FMNIST:\n",
    "    # Generate transformations for train  and set\n",
    "    train_transform_operations = [transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    test_transform_operations = [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "\n",
    "    # Data Transformations\n",
    "    random_crop     = True\n",
    "    random_erasing  = True\n",
    "    convert_to_RGB  = True\n",
    "\n",
    "    if random_crop:\n",
    "        train_transform_operations.insert(0, transforms.RandomCrop(28, padding=4))\n",
    "    if random_erasing:\n",
    "        train_transform_operations.append(transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=\"random\", inplace=False))\n",
    "    if convert_to_RGB:\n",
    "        to_rgb = transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
    "        train_transform_operations.append(to_rgb)\n",
    "        test_transform_operations.append(to_rgb)\n",
    "\n",
    "    # Data Loaders\n",
    "    percentage_validation_set = 10\n",
    "    batch_size = 256\n",
    "\n",
    "    # Train anc Val Set\n",
    "    train_transform = transforms.Compose(train_transform_operations)\n",
    "\n",
    "    vis_train_dataset = datasets.FashionMNIST(\n",
    "        root=data_root, train=True, transform=ToTensor(), download=True\n",
    "    )\n",
    "\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(\n",
    "        root=data_root, train=True, transform=train_transform, download=True\n",
    "    )\n",
    "\n",
    "    train_set_length = int(len(train_dataset) * (100 - percentage_validation_set) / 100)\n",
    "    val_set_length = int(len(train_dataset) - train_set_length)\n",
    "\n",
    "    # Randomly split a dataset into non-overlapping new datasets of given lengths. Fix the generator for reproducible results\n",
    "    train_set, val_set = torch.utils.data.random_split(train_dataset, (train_set_length, val_set_length), generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    # set shuffle=True to have the data reshuffled at every epoch \n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    # Test Set\n",
    "    test_transform = transforms.Compose(test_transform_operations)\n",
    "\n",
    "    test_set = datasets.FashionMNIST(\n",
    "        root=data_root, train=False, transform=test_transform, download=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "elif experiment_dataset == ExperimentDataset.CIFAR10:\n",
    "\n",
    "    transform = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', download=True, train=True, transform=transform)\n",
    "\n",
    "    # Get all training targets and count the number of class instances\n",
    "    targets = np.array(train_dataset.targets)\n",
    "    cl, class_counts = np.unique(targets, return_counts=True)\n",
    "    nb_classes = len(cl)\n",
    "    print(class_counts)\n",
    "\n",
    "    percentage_validation_set = 10\n",
    "    batch_size = 4\n",
    "    batch_size = 256\n",
    "\n",
    "    ## ---------------------------- ##\n",
    "    ## SPLIT TRAIN / VAL ---------- ##\n",
    "    ## ---------------------------- ##\n",
    "    train_set_length = int(len(train_dataset) * (100 - percentage_validation_set) / 100)\n",
    "    val_set_length = int(len(train_dataset) - train_set_length)\n",
    "\n",
    "    # Randomly split a dataset into non-overlapping new datasets of given lengths. Fix the generator for reproducible results\n",
    "    train_set, val_set = torch.utils.data.random_split(train_dataset, (train_set_length, val_set_length), generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    # set shuffle=True to have the data reshuffled at every epoch \n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    ## ---------------------------- ##\n",
    "    ## SPLIT TEST ----------------- ##\n",
    "    ## ---------------------------- ##\n",
    "    test_transform_operations = [\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "    ]\n",
    "\n",
    "    test_transform = Compose(test_transform_operations)\n",
    "    test_set = datasets.CIFAR10(root='./data', train=False, transform=test_transform, download=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "elif experiment_dataset == ExperimentDataset.INTELIMAGE:\n",
    "\n",
    "    percentage_validation_set = 10\n",
    "    batch_size = 4\n",
    "\n",
    "    data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    }\n",
    "\n",
    "    data_dir = './data/intelimage/intelimage/IntelImageClassification'\n",
    "\n",
    "    ## ---------------------------- ##\n",
    "    ## SPLIT TRAIN / VAL ---------- ##\n",
    "    ## ---------------------------- ##\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "    train_dataset = image_datasets['train']\n",
    "\n",
    "    train_set_length = int(len(train_dataset) * (100 - percentage_validation_set) / 100)\n",
    "    val_set_length = int(len(train_dataset) - train_set_length)\n",
    "\n",
    "    # Randomly split a dataset into non-overlapping new datasets of given lengths. Fix the generator for reproducible results\n",
    "    train_set, val_set = torch.utils.data.random_split(train_dataset, (train_set_length, val_set_length), generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    # set shuffle=True to have the data reshuffled at every epoch \n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    ## ---------------------------- ##\n",
    "    ## SPLIT TEST ----------------- ##\n",
    "    ## ---------------------------- ##\n",
    "    test_set = image_datasets['test']\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdSKYrXHHjgP",
    "outputId": "87fe6f2e-40bf-4839-ba09-c175d1c31759"
   },
   "outputs": [],
   "source": [
    "# Define the class for neural network model with He Initialization\n",
    "# Best with: ReLU\n",
    "class Net_He(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, Layers):\n",
    "        super(Net_He, self).__init__()\n",
    "        self.hidden = nn.ModuleList()\n",
    "\n",
    "        for input_size, output_size in zip(Layers, Layers[1:]):\n",
    "            linear = nn.Linear(input_size, output_size)\n",
    "            torch.nn.init.kaiming_uniform_(linear.weight, nonlinearity='relu')\n",
    "            self.hidden.append(linear)\n",
    "\n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        L = len(self.hidden)\n",
    "        for (l, linear_transform) in zip(range(L), self.hidden):\n",
    "            if l < L - 1:\n",
    "                x = F.relu(linear_transform(x))\n",
    "            else:\n",
    "                x = nn.Softmax(dim=1)(x)\n",
    "        return x\n",
    "\n",
    "if experiment_model == ExperimentModel.MLP:\n",
    "    # Set the size of the neural network\n",
    "    input_size = 3*32*32\n",
    "    output_size = 10\n",
    "    #layers = [input_size, 4000, 1000, 4000, output_size]\n",
    "    layers = [input_size, 1024, output_size]\n",
    "\n",
    "    # Determine the learning rate\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = Net_He(layers)\n",
    "\n",
    "    # Determine the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Determine the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_trainable_parameters = count_parameters(model, only_trainable_parameters=True)\n",
    "\n",
    "    # Load the model on the GPU if available\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    print(f\"Loaded MLP with {num_trainable_parameters} trainable parameters (GPU: {use_cuda})\")\n",
    "\n",
    "elif experiment_model == ExperimentModel.RESNET:\n",
    "    num_epochs = 30\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "    weight_decay = 0\n",
    "    nesterov = True\n",
    "    \n",
    "    model = models.resnet18(pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, len(classes))\n",
    "\n",
    "    num_trainable_parameters = count_parameters(model, only_trainable_parameters=True)\n",
    "\n",
    "    # Load the model on the GPU if available\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay,\n",
    "        nesterov=nesterov\n",
    "    )\n",
    "\n",
    "    print(f\"Loaded ResNet18 with {num_trainable_parameters} trainable parameters (GPU: {use_cuda})\")\n",
    "\n",
    "elif experiment_model == ExperimentModel.VGG:\n",
    "    num_epochs = 30\n",
    "    #prereq\n",
    "    class_names = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street'] \n",
    "\n",
    "    # Load the pretrained model from pytorch\n",
    "    model = models.vgg16_bn()\n",
    "    model.load_state_dict(torch.load(\"./data/intelimage/vgg16_bn.pth\"))\n",
    "    print(model.classifier[6].out_features) # 1000 \n",
    "\n",
    "\n",
    "    # Freeze training for all layers\n",
    "    for param in model.features.parameters():\n",
    "        param.require_grad = False\n",
    "\n",
    "    # Newly created modules have require_grad=True by default\n",
    "    num_features = model.classifier[6].in_features\n",
    "    features = list(model.classifier.children())[:-1] # Remove last layer\n",
    "    features.extend([nn.Linear(num_features, len(class_names))]) # Add our layer with 6 outputs\n",
    "    model.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "    # Load the model on the GPU if available\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    #vgg16 = vgg16.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that only parameters of final layer are being optimized as\n",
    "    # opposed to before.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    num_trainable_parameters = count_parameters(model, only_trainable_parameters=True)\n",
    "    print(f\"Loaded VGG with {num_trainable_parameters} trainable parameters (GPU: {use_cuda})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgsN750oHjgU"
   },
   "source": [
    "## Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXY_YBcaHjgV"
   },
   "source": [
    "#### FMNIST_IMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfZE5r7FHjgV"
   },
   "outputs": [],
   "source": [
    "if experiment_dataset == ExperimentDataset.FMNIST_IMB70:\n",
    "    # Generate transformations for train  and set\n",
    "    train_transform_operations = [transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    test_transform_operations = [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "\n",
    "    # Data Transformations\n",
    "    random_crop     = True\n",
    "    random_erasing  = True\n",
    "    convert_to_RGB  = True\n",
    "\n",
    "    if random_crop:\n",
    "        train_transform_operations.insert(0, transforms.RandomCrop(28, padding=4))\n",
    "    if random_erasing:\n",
    "        train_transform_operations.append(transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=\"random\", inplace=False))\n",
    "    if convert_to_RGB:\n",
    "        to_rgb = transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
    "        train_transform_operations.append(to_rgb)\n",
    "        test_transform_operations.append(to_rgb)\n",
    "\n",
    "    # Data Loaders\n",
    "    percentage_validation_set = 10\n",
    "    batch_size = 256\n",
    "\n",
    "    # Train anc Val Set\n",
    "    train_transform = transforms.Compose(train_transform_operations)\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(\n",
    "        root=data_root, train=True, transform=train_transform, download=True\n",
    "    )\n",
    "\n",
    "    # Test Set\n",
    "    test_transform = transforms.Compose(test_transform_operations)\n",
    "\n",
    "    test_set = datasets.FashionMNIST(\n",
    "        root=data_root, train=False, transform=test_transform, download=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Get all training targets and count the number of class instances\n",
    "    targets = np.array(train_dataset.targets)\n",
    "    cl, class_counts = np.unique(targets, return_counts=True)\n",
    "    nb_classes = len(cl)\n",
    "    print(class_counts)\n",
    "\n",
    "    # Create artificial imbalanced class counts\n",
    "    imbal_class_counts = [1800, 6000] * 5\n",
    "\n",
    "    class_indices = [np.where(targets == i)[0] for i in range(nb_classes)]\n",
    "\n",
    "    imbal_class_indices = [class_idx[:class_count] for class_idx, class_count in zip(class_indices, imbal_class_counts)]\n",
    "    imbal_class_indices = np.hstack(imbal_class_indices)\n",
    "\n",
    "\n",
    "    train_dataset.targets = targets[imbal_class_indices]\n",
    "    train_dataset.data = train_dataset.data[imbal_class_indices]\n",
    "\n",
    "    \n",
    "    # Get all training targets and count the number of class instances\n",
    "    targets = np.array(train_dataset.targets)\n",
    "    cl, class_counts = np.unique(targets, return_counts=True)\n",
    "    nb_classes = len(cl)\n",
    "    print(class_counts)\n",
    "\n",
    "\n",
    "    train_set_length = int(len(train_dataset) * (100 - percentage_validation_set) / 100)\n",
    "    val_set_length = int(len(train_dataset) - train_set_length)\n",
    "\n",
    "    # Randomly split a dataset into non-overlapping new datasets of given lengths. Fix the generator for reproducible results\n",
    "    train_set, val_set = torch.utils.data.random_split(train_dataset, (train_set_length, val_set_length), generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    # set shuffle=True to have the data reshuffled at every epoch \n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "if experiment_dataset == ExperimentDataset.FMNIST_IMB90:\n",
    "    # Generate transformations for train  and set\n",
    "    train_transform_operations = [transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    test_transform_operations = [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "\n",
    "    # Data Transformations\n",
    "    random_crop     = True\n",
    "    random_erasing  = True\n",
    "    convert_to_RGB  = True\n",
    "\n",
    "    if random_crop:\n",
    "        train_transform_operations.insert(0, transforms.RandomCrop(28, padding=4))\n",
    "    if random_erasing:\n",
    "        train_transform_operations.append(transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=\"random\", inplace=False))\n",
    "    if convert_to_RGB:\n",
    "        to_rgb = transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
    "        train_transform_operations.append(to_rgb)\n",
    "        test_transform_operations.append(to_rgb)\n",
    "\n",
    "    # Data Loaders\n",
    "    percentage_validation_set = 10\n",
    "    batch_size = 256\n",
    "\n",
    "    # Train anc Val Set\n",
    "    train_transform = transforms.Compose(train_transform_operations)\n",
    "   \n",
    "    train_dataset = datasets.FashionMNIST(\n",
    "        root=data_root, train=True, transform=train_transform, download=True\n",
    "    )\n",
    "\n",
    "    # Test Set\n",
    "    test_transform = transforms.Compose(test_transform_operations)\n",
    "\n",
    "    test_set = datasets.FashionMNIST(\n",
    "        root=data_root, train=False, transform=test_transform, download=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Get all training targets and count the number of class instances\n",
    "    targets = np.array(train_dataset.targets)\n",
    "    cl, class_counts = np.unique(targets, return_counts=True)\n",
    "    nb_classes = len(cl)\n",
    "    print(class_counts)\n",
    "\n",
    "    # Create artificial imbalanced class counts\n",
    "    imbal_class_counts = [600, 6000] * 5\n",
    "\n",
    "    class_indices = [np.where(targets == i)[0] for i in range(nb_classes)]\n",
    "\n",
    "    imbal_class_indices = [class_idx[:class_count] for class_idx, class_count in zip(class_indices, imbal_class_counts)]\n",
    "    imbal_class_indices = np.hstack(imbal_class_indices)\n",
    "\n",
    "\n",
    "    train_dataset.targets = targets[imbal_class_indices]\n",
    "    train_dataset.data = train_dataset.data[imbal_class_indices]\n",
    "\n",
    "    \n",
    "    # Get all training targets and count the number of class instances\n",
    "    targets = np.array(train_dataset.targets)\n",
    "    cl, class_counts = np.unique(targets, return_counts=True)\n",
    "    nb_classes = len(cl)\n",
    "    print(class_counts)\n",
    "\n",
    "\n",
    "    train_set_length = int(len(train_dataset) * (100 - percentage_validation_set) / 100)\n",
    "    val_set_length = int(len(train_dataset) - train_set_length)\n",
    "\n",
    "    # Randomly split a dataset into non-overlapping new datasets of given lengths. Fix the generator for reproducible results\n",
    "    train_set, val_set = torch.utils.data.random_split(train_dataset, (train_set_length, val_set_length), generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    # set shuffle=True to have the data reshuffled at every epoch \n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-5QZGodHjgY"
   },
   "source": [
    "#### INTEL_IMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXxeYjClHjgY",
    "outputId": "31534b9d-0d8d-4d4b-d25a-c7cc076dc2a2"
   },
   "outputs": [],
   "source": [
    "if experiment_dataset == ExperimentDataset.INTELIMAGE_IMB90:\n",
    "\n",
    "    percentage_validation_set = 10\n",
    "    batch_size = 4\n",
    "\n",
    "    data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    }\n",
    "\n",
    "    data_dir = './data/intelimage/intelimage/IntelImageClassification'\n",
    "\n",
    "    ## ---------------------------- ##\n",
    "    ## SPLIT TRAIN / VAL ---------- ##\n",
    "    ## ---------------------------- ##\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "    train_dataset = image_datasets['train']\n",
    "\n",
    "    # Get all training targets and count the number of class instances\n",
    "    targets = np.array(train_dataset.targets)\n",
    "    cl, class_counts = np.unique(targets, return_counts=True)\n",
    "    nb_classes = len(cl)\n",
    "    print(class_counts)\n",
    "    \n",
    "    # Create artificial imbalanced class counts\n",
    "    imbal_class_counts = [200, 2000] * 3\n",
    "\n",
    "    class_indices = [np.where(targets == i)[0] for i in range(nb_classes)]\n",
    "\n",
    "    imbal_class_indices = [class_idx[:class_count] for class_idx, class_count in zip(class_indices, imbal_class_counts)]\n",
    "    imbal_class_indices = np.hstack(imbal_class_indices)\n",
    "\n",
    "\n",
    "    train_dataset.targets = targets[imbal_class_indices]\n",
    "\n",
    "\n",
    "\n",
    "   # train_dataset = train_dataset[imbal_class_indices]\n",
    "\n",
    "    \n",
    "    # Get all training targets and count the number of class instances\n",
    "    targets = np.array(train_dataset.targets)\n",
    "    cl, class_counts = np.unique(targets, return_counts=True)\n",
    "    nb_classes = len(cl)\n",
    "    print(class_counts)\n",
    "\n",
    "\n",
    "    train_set_length = int(len(train_dataset) * (100 - percentage_validation_set) / 100)\n",
    "    val_set_length = int(len(train_dataset) - train_set_length)\n",
    "\n",
    "    # Randomly split a dataset into non-overlapping new datasets of given lengths. Fix the generator for reproducible results\n",
    "    train_set, val_set = torch.utils.data.random_split(train_dataset, (train_set_length, val_set_length), generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    # set shuffle=True to have the data reshuffled at every epoch \n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    " \n",
    "    ## ---------------------------- ##\n",
    "    ## SPLIT TEST ----------------- ##\n",
    "    ## ---------------------------- ##\n",
    "    test_set = image_datasets['test']\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ldc4-4tHjga"
   },
   "outputs": [],
   "source": [
    "imbalanced_enabled = False\n",
    "if imbalanced_enabled:\n",
    "\n",
    "    from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "    transform = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10( root='./data', download=True, train=True, transform=transform)\n",
    "\n",
    "    # Get all training targets and count the number of class instances\n",
    "    targets = np.array(train_dataset.targets)\n",
    "    cl, class_counts = np.unique(targets, return_counts=True)\n",
    "    nb_classes = len(cl)\n",
    "    print(class_counts)\n",
    "\n",
    "    # Create artificial imbalanced class counts\n",
    "    imbal_class_counts = [500, 5000] * 5\n",
    "\n",
    "    class_indices = [np.where(targets == i)[0] for i in range(nb_classes)]\n",
    "\n",
    "    imbal_class_indices = [class_idx[:class_count] for class_idx, class_count in zip(class_indices, imbal_class_counts)]\n",
    "    imbal_class_indices = np.hstack(imbal_class_indices)\n",
    "\n",
    "\n",
    "    train_dataset.targets = targets[imbal_class_indices]\n",
    "    train_dataset.data = train_dataset.data[imbal_class_indices]\n",
    "\n",
    "    \n",
    "    # Get all training targets and count the number of class instances\n",
    "    targets = np.array(train_dataset.targets)\n",
    "    classes, class_counts = np.unique(targets, return_counts=True)\n",
    "    nb_classes = len(classes)\n",
    "    print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAAJxjlPHjga"
   },
   "source": [
    "### Data Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-swsVvBHjga"
   },
   "outputs": [],
   "source": [
    "datavis_enabled = False\n",
    "if datavis_enabled:\n",
    "\n",
    "    labels_map = {\n",
    "        0: \"T-Shirt\",\n",
    "        1: \"Trouser\",\n",
    "        2: \"Pullover\",\n",
    "        3: \"Dress\",\n",
    "        4: \"Coat\",\n",
    "        5: \"Sandal\",\n",
    "        6: \"Shirt\",\n",
    "        7: \"Sneaker\",\n",
    "        8: \"Bag\",\n",
    "        9: \"Ankle Boot\",\n",
    "    }\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    cols, rows = 3, 3\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        sample_idx = torch.randint(len(vis_train_dataset), size=(1,)).item()\n",
    "        img, label = vis_train_dataset[sample_idx]\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        plt.title(labels_map[label])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.squeeze(), cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "\n",
    "    # example of loading the fashion mnist dataset\n",
    " \n",
    "    # plot first few images\n",
    "    for i in range(9):\n",
    "        # define subplot\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(train_set.dataset.data[i], cmap=plt.get_cmap('gray'))\n",
    "    # show the figure\n",
    "    plt.show()\n",
    "\n",
    "    from collections import Counter\n",
    "    train_classes = [label for img, label in train_dataset]\n",
    "    \n",
    "    total_count = Counter(train_classes)\n",
    "    class_ratio = {}\n",
    "\n",
    "    for k, v in total_count.items():\n",
    "        class_ratio[k] = v * 100 / train_dataset.data.size()[0]\n",
    "    \n",
    "\n",
    "    class_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GHDkIH3Hjgb"
   },
   "source": [
    "## Train Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UXtNpplHjgc"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    classes: dict,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: nn.Module,\n",
    "    epoch: int,\n",
    "    num_iteration: int,\n",
    "    use_cuda: bool,\n",
    "    tensorboard_writer: torch.utils.tensorboard.SummaryWriter):\n",
    "  \"\"\" Train a given model\n",
    "  Args:\n",
    "      model (nn.Module): model to train.\n",
    "      classes (dict): dictionnary containing the classes and their indice.\n",
    "      data_loader (torch.utils.data.DataLoader): data loader with the data to train the model on.\n",
    "      criterion (nn.Module): loss function.\n",
    "      optimizer (nn.Module): optimizer function.\n",
    "      epoch (int): epoch of training.\n",
    "      num_iteration (int): number of iterations since the beginning of the training.\n",
    "      use_cuda (bool): boolean to decide if cuda should be used.\n",
    "      tensorboard_writer (torch.utils.tensorboard.SummaryWriter): writer to write the metrics in tensorboard.\n",
    "  Returns:\n",
    "      num_iteration (int): number of iterations since the beginning of the training (increased during the training).\n",
    "      loss (float): final loss\n",
    "      accuracy_top1 (float): final accuracy top1\n",
    "      accuracy_top5 (float): final accuracy top5\n",
    "      confidence_mean (float): mean confidence\n",
    "  \"\"\"\n",
    "  # Switch the model to train mode\n",
    "  model.train()\n",
    "\n",
    "  # Initialize the trackers for the loss and the accuracy\n",
    "  loss_tracker = MetricTracker()\n",
    "  accuracy_top1_tracker = MetricTracker()\n",
    "  accuracy_top5_tracker = MetricTracker()\n",
    "  confidence_tracker = MetricTracker()\n",
    "\n",
    "  # Initialize confusing matrix\n",
    "  confusion_matrix_tracker = ConfusionMatrix(classes)\n",
    "\n",
    "  # create BackgroundGenerator and wrap it in tqdm progress bar\n",
    "  progress_bar = tqdm(\n",
    "      BackgroundGenerator(data_loader, max_prefetch=32), total=len(data_loader)\n",
    "  )\n",
    "\n",
    "  for i, data in enumerate(progress_bar):\n",
    "      inputs, targets = data\n",
    "\n",
    "      # Save the inputs to the disk\n",
    "      # img_grid = torchvision.utils.make_grid(inputs)\n",
    "      # torchvision.utils.save_image(img_grid,\"inputs.jpg\")\n",
    "\n",
    "      if use_cuda:\n",
    "          inputs = inputs.cuda()\n",
    "          targets = targets.cuda()\n",
    " \n",
    "      # Forward pass\n",
    "      if experiment_model == ExperimentModel.MLP:\n",
    "        outputs = model(inputs.view(-1, 3*32*32))\n",
    "      else:\n",
    "        outputs = model(inputs)\n",
    "\n",
    "      #\n",
    "      \n",
    "      loss = criterion(outputs, targets)\n",
    "      confidence, prediction = outputs.topk(dim=1, k=5)\n",
    "\n",
    "      # Backward pass and optimizer step\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Track loss, accuracy and confidence\n",
    "      loss_tracker.update(loss.item())\n",
    "      accuracy_top1_tracker.update(\n",
    "          (prediction[:, 0] == targets).sum().item(), targets.numel()\n",
    "      )\n",
    "      accuracy_top5_tracker.update(\n",
    "          (prediction[:, :5] == targets[:, None]).sum().item(), targets.numel()\n",
    "      )\n",
    "      confidence_tracker.update(confidence[:, 0].sum().item(), targets.numel())\n",
    "\n",
    "      # Update the confusion matrix\n",
    "      confusion_matrix_tracker.update_confusion_matrix(targets.cpu(), prediction[:, 0].cpu())\n",
    "\n",
    "      # Add the new values to the tensorboard summary writer\n",
    "      tensorboard_writer.add_scalar(\"loss\", loss_tracker.average, num_iteration)\n",
    "      tensorboard_writer.add_scalar(\n",
    "          \"accuracy_top1\", accuracy_top1_tracker.average, num_iteration\n",
    "      )\n",
    "      tensorboard_writer.add_scalar(\n",
    "          \"accuracy_top5\", accuracy_top5_tracker.average, num_iteration\n",
    "      )\n",
    "      # tensorboard_writer.add_scalar(\n",
    "      #     \"confidence_mean\", confidence_tracker.average, num_iteration\n",
    "      # )\n",
    "\n",
    "      # Update the progress_bar information\n",
    "      progress_bar.set_description(f\"Epoch {epoch + 1}/{num_epochs} Train\")\n",
    "      progress_bar.set_postfix(\n",
    "          loss=f\"{loss_tracker.average:05.5f}\",\n",
    "          accuracy_top1=f\"{100 * accuracy_top1_tracker.average:05.2f}\",\n",
    "          accuracy_top5=f\"{100 * accuracy_top5_tracker.average:05.2f}\",\n",
    "      )\n",
    "\n",
    "      # Increment num_iteration on all iterations except the last,\n",
    "      # so that the evaluation is logged to the correct iteration\n",
    "      if i < len(data_loader) - 1:\n",
    "          num_iteration += 1\n",
    "\n",
    "  # Add the normalized confusion matrix to tensorboard and flush it\n",
    "  tensorboard_writer.add_figure(\n",
    "      \"confusion_matrix\", confusion_matrix_tracker.plot_confusion_matrix(normalize=True), num_iteration\n",
    "  )\n",
    "  tensorboard_writer.flush()\n",
    "\n",
    "  return (\n",
    "      num_iteration,\n",
    "      loss_tracker.average,\n",
    "      accuracy_top1_tracker.average,\n",
    "      accuracy_top5_tracker.average,\n",
    "      confidence_tracker.average,\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeXucehzHjgd"
   },
   "source": [
    "## Test Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFIgESRDHjgd"
   },
   "outputs": [],
   "source": [
    "def test(\n",
    "    model: nn.Module,\n",
    "    classes: dict,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    epoch: int,\n",
    "    num_iteration: int,\n",
    "    use_cuda: bool,\n",
    "    tensorboard_writer: torch.utils.tensorboard.SummaryWriter,\n",
    "    name_step: str,\n",
    "):\n",
    "    \"\"\" Test a given model\n",
    "    Args:\n",
    "        model (nn.Module): model to test.\n",
    "        classes (dict): dictionnary containing the classes and their indice.\n",
    "        data_loader (torch.utils.data.DataLoader): data loader with the data to test the model on.\n",
    "        criterion (nn.Module): loss function.\n",
    "        epoch (int): epoch of training corresponding to the model.\n",
    "        num_iteration (int): number of iterations since the beginning of the training corresponding to the model.\n",
    "        use_cuda (bool): boolean to decide if cuda should be used.\n",
    "        tensorboard_writer (torch.utils.tensorboard.SummaryWriter): writer to write the metrics in tensorboard.\n",
    "        name_step (str): name of the step to write it in the description of the progress_bar\n",
    "    Returns:\n",
    "        loss (float): final loss\n",
    "        accuracy_top1 (float): final accuracy top1\n",
    "        accuracy_top5 (float): final accuracy top5\n",
    "        confidence_mean (float): mean confidence\n",
    "    \"\"\"\n",
    "    # Switch the model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the trackers for the loss and the accuracy\n",
    "    loss_tracker = MetricTracker()\n",
    "    accuracy_top1_tracker = MetricTracker()\n",
    "    accuracy_top5_tracker = MetricTracker()\n",
    "    confidence_tracker = MetricTracker()\n",
    "\n",
    "    # Initialize confusing matrix\n",
    "    confusion_matrix_tracker = ConfusionMatrix(classes)\n",
    "\n",
    "    # create BackgroundGenerator and wrap it in tqdm progress bar\n",
    "    progress_bar = tqdm(\n",
    "        BackgroundGenerator(data_loader, max_prefetch=32), total=len(data_loader)\n",
    "    )\n",
    "\n",
    "    for data in progress_bar:\n",
    "        inputs, targets = data\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "        # forward pass\n",
    "        # Forward pass\n",
    "        if experiment_model == ExperimentModel.MLP:\n",
    "          outputs = model(inputs.view(-1, 3*32*32))\n",
    "        else:\n",
    "          outputs = model(inputs)\n",
    "         \n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        confidence, prediction = outputs.topk(dim=1, k=5)\n",
    "\n",
    "        # Track loss, accuracy and confidence\n",
    "        loss_tracker.update(loss.item())\n",
    "        accuracy_top1_tracker.update(\n",
    "            (prediction[:, 0] == targets).sum().item(), targets.numel()\n",
    "        )\n",
    "        accuracy_top5_tracker.update(\n",
    "            (prediction[:, :5] == targets[:, None]).sum().item(), targets.numel()\n",
    "        )\n",
    "        confidence_tracker.update(confidence[:, 0].sum().item(), targets.numel())\n",
    "\n",
    "        # Update the confusion matrix\n",
    "        confusion_matrix_tracker.update_confusion_matrix(targets.cpu(), prediction[:, 0].cpu())\n",
    "\n",
    "        # Update the progress_bar information\n",
    "        progress_bar.set_description(f\"Epoch {epoch + 1}/{num_epochs} {name_step}\")\n",
    "        progress_bar.set_postfix(\n",
    "            loss=f\"{loss_tracker.average:05.5f}\",\n",
    "            accuracy_top1=f\"{100 * accuracy_top1_tracker.average:05.2f}\",\n",
    "            accuracy_top5=f\"{100 * accuracy_top5_tracker.average:05.2f}\",\n",
    "        )\n",
    "\n",
    "    # Add the new values to the tensorboard summary writer\n",
    "    tensorboard_writer.add_scalar(\"loss\", loss_tracker.average, num_iteration)\n",
    "    tensorboard_writer.add_scalar(\n",
    "        \"accuracy_top1\", accuracy_top1_tracker.average, num_iteration\n",
    "    )\n",
    "    tensorboard_writer.add_scalar(\n",
    "        \"accuracy_top5\", accuracy_top5_tracker.average, num_iteration\n",
    "    )\n",
    "    # tensorboard_writer.add_scalar(\n",
    "    #     \"confidence_mean\", confidence_tracker.average, num_iteration\n",
    "    # )\n",
    "\n",
    "    tensorboard_writer.add_figure(\n",
    "        \"confusion_matrix\", confusion_matrix_tracker.plot_confusion_matrix(normalize=True), num_iteration\n",
    "    )\n",
    "    tensorboard_writer.flush()\n",
    "\n",
    "    return (\n",
    "        loss_tracker.average,\n",
    "        accuracy_top1_tracker.average,\n",
    "        accuracy_top5_tracker.average,\n",
    "        confidence_tracker.average,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCu23lidHjge"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    current_epoch: int,\n",
    "    num_iteration: int,\n",
    "    best_accuracy: float,\n",
    "    model_state_dict: dict,\n",
    "    optimizer_state_dict: dict,\n",
    "    is_best: bool,\n",
    "    experiment_path: str,\n",
    "    checkpoint_filename: str = \"checkpoint.pth.tar\",\n",
    "    best_filename: str = \"model_best.pth.tar\",\n",
    "):\n",
    "    \"\"\" Save the checkpoint and the best model to the disk\n",
    "    Args:\n",
    "        current_epoch (int): current epoch of the training.\n",
    "        num_iteration (int): number of iterations since the beginning of the training.\n",
    "        best_accuracy (float): last best accuracy obtained during the training.\n",
    "        model_state_dict (dict): dictionary containing information about the model's state.\n",
    "        optimizer_state_dict (dict): dictionary containing information about the optimizer's state.\n",
    "        is_best (bool): boolean to save the current model as the new best model.\n",
    "        experiment_path (str): path to the directory where to save the checkpoints and the best model.\n",
    "        checkpoint_filename (str: \"checkpoint.pth.tar\"): filename to give to the checkpoint.\n",
    "        best_filename (str: \"model_best.pth.tar\"):  filename to give to the best model's checkpoint.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f'Saving checkpoint{f\" and new best model (best accuracy: {100 * best_accuracy:05.2f})\" if is_best else f\"\"}...'\n",
    "    )\n",
    "    checkpoint_filepath = os.path.join(experiment_path, checkpoint_filename)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": current_epoch,\n",
    "            \"num_iteration\": num_iteration,\n",
    "            \"best_accuracy\": best_accuracy,\n",
    "            \"model_state_dict\": model_state_dict,\n",
    "            \"optimizer_state_dict\": optimizer_state_dict,\n",
    "        },\n",
    "        checkpoint_filepath,\n",
    "    )\n",
    "    if is_best:\n",
    "        shutil.copyfile(\n",
    "            checkpoint_filepath, os.path.join(experiment_path, best_filename),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOyWQOj8Hjge"
   },
   "source": [
    "## Train - Test Pipeline ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KX78kWSMHjgf",
    "outputId": "2d3fc414-7180-41f9-f420-9fb64f293678"
   },
   "outputs": [],
   "source": [
    "# --- MODEL TRAINING & TESTING ---\n",
    "start_num_iteration = 0\n",
    "start_epoch = 0\n",
    "best_accuracy = 0.0\n",
    "epochs_without_improvement = 0\n",
    "purge_step = None\n",
    "num_epochs = 30\n",
    "patience = -1\n",
    " \n",
    "# Restore the last checkpoint if available\n",
    "checkpoint_filepath = os.path.join(experiment_path, \"checkpoint.pth.tar\")\n",
    "if os.path.exists(checkpoint_filepath):\n",
    "    print(f\"Restoring last checkpoint from {checkpoint_filepath}...\")\n",
    "    checkpoint = torch.load(checkpoint_filepath)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    start_num_iteration = checkpoint[\"num_iteration\"] + 1\n",
    "    best_accuracy = checkpoint[\"best_accuracy\"]\n",
    "    purge_step = start_num_iteration\n",
    "    print(\n",
    "        f\"Last checkpoint restored. Starting at epoch {start_epoch + 1} with best accuracy at {100 * best_accuracy:05.3f}.\"\n",
    "    )\n",
    "\n",
    "# Create the tensorboard summary writers for training and validation steps\n",
    "train_writer = SummaryWriter(\n",
    "    os.path.join(experiment_path, \"train\"), purge_step=purge_step\n",
    ")\n",
    "valid_writer = SummaryWriter(\n",
    "    os.path.join(experiment_path, \"valid\"), purge_step=purge_step\n",
    ")\n",
    "test_writer = SummaryWriter(\n",
    "    os.path.join(experiment_path, \"test\"), purge_step=purge_step\n",
    ")\n",
    "\n",
    "# main training loop\n",
    "num_iteration = start_num_iteration\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "    # --- TRAIN ---\n",
    "\n",
    "    num_iteration, _, _, _, _ = train(\n",
    "        model=model,\n",
    "        classes=classes,\n",
    "        data_loader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch,\n",
    "        num_iteration=num_iteration,\n",
    "        use_cuda=use_cuda,\n",
    "        tensorboard_writer=train_writer\n",
    "    )\n",
    "\n",
    "    # --- VALID ---\n",
    "\n",
    "    is_best = False\n",
    "\n",
    "    _, valid_accuracy_top1, _, _ = test(\n",
    "        model=model,\n",
    "        classes=classes,\n",
    "        data_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        epoch=epoch,\n",
    "        num_iteration=num_iteration,\n",
    "        use_cuda=use_cuda,\n",
    "        tensorboard_writer=valid_writer,\n",
    "        name_step=\"Valid\",\n",
    "    )\n",
    "\n",
    "    # Save the best model\n",
    "    if valid_accuracy_top1 > best_accuracy:\n",
    "        is_best = True\n",
    "        best_accuracy = valid_accuracy_top1\n",
    "        # Re-initialize epochs_without_improvement\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "    # Early stopping\n",
    "    elif (patience >= 0) and (epochs_without_improvement >= patience):\n",
    "        print(\n",
    "            f\"No improvement for the last {epochs_without_improvement} epochs, stopping the training (best accuracy: {100 * best_accuracy:05.2f}).\"\n",
    "        )\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    save_checkpoint(\n",
    "        current_epoch=epoch,\n",
    "        num_iteration=num_iteration,\n",
    "        best_accuracy=best_accuracy,\n",
    "        model_state_dict=model.state_dict(),\n",
    "        optimizer_state_dict=optimizer.state_dict(),\n",
    "        is_best=is_best,\n",
    "        experiment_path=experiment_path,\n",
    "    )\n",
    "\n",
    "    # increment num_iteration after evaluation for the next epoch of training\n",
    "    num_iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2Ot7CXOHjgg"
   },
   "source": [
    "/home/one/Development/AdaptiveCostSensitiveClassification/FMNIST/resnet/experiment/220604-160336_vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeKtIVyZHjgg"
   },
   "outputs": [],
   "source": [
    "# --- TEST ---\n",
    "\n",
    "# Restore the best model to test it\n",
    "best_model_filepath = os.path.join(experiment_path, \"model_best.pth.tar\")\n",
    "if os.path.exists(best_model_filepath):\n",
    "    print(f\"Loading best model from {best_model_filepath}...\")\n",
    "    checkpoint = torch.load(best_model_filepath)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    best_accuracy = checkpoint[\"best_accuracy\"]\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    num_iteration = checkpoint[\"num_iteration\"]\n",
    "\n",
    "_, test_accuracy_top1, _, _ = test(\n",
    "    model=model,\n",
    "    classes=classes,\n",
    "    data_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    epoch=epoch,\n",
    "    num_iteration=num_iteration,\n",
    "    use_cuda=use_cuda,\n",
    "    tensorboard_writer=test_writer,\n",
    "    name_step=\"Test\",\n",
    ")\n",
    "\n",
    "# Print final accuracy of the best model on the test set\n",
    "print(\n",
    "    f\"Best ResNet18 model has an accuracy of {100 * test_accuracy_top1:05.2f} on the Fashion MNIST test set.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaM_Sx6SHjgg"
   },
   "source": [
    "## TENSORBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K57faWjCHjgh"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kjz5ESo9Hjgh"
   },
   "source": [
    "%tensorboard --logdir {\"experiment/220604-111621_mlp\"}  -- MLP - Orig - CIFAR10\n",
    "%tensorboard --logdir {\"experiment/220604-134904_resnet18\"} -- ResNet - Orig - FMNIST\n",
    "%tensorboard --logdir {\"experiment/220604-160336_vgg16\"} -- VGG16 - Orig - IntelImage\n",
    "\n",
    "\n",
    "%tensorboard --logdir {\"experiment/220604-194320_resnet18\"} -- ResNet - 70 - FMNIST \n",
    "%tensorboard --logdir {\"experiment/220604-203020_resnet18\"} -- ResNet - 90 - FMNIST \n",
    "\n",
    "%tensorboard --logdir {\"experiment/220604-204100_resnet18\"} -- ResNet - Orig - CIFAR10 \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9W4bx20Hjgh"
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "%tensorboard --logdir {\"experiment/220604-204100_resnet18\"}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nzRzH8VHjgi"
   },
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyoVR6QMHjgi"
   },
   "source": [
    "Epoch 23/30 Test: 100%|██████████| 40/40 [00:01<00:00, 31.11it/s, accuracy_top1=89.27, accuracy_top5=99.72, loss=0.40885]\n",
    "Best ResNet18 model has an accuracy of 89.27 on the Fashion MNIST test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pipeline.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "ae6d9569a3bc2e57e6d4ae35d68f60145f9de725e53ee96ffff215d4f335ae07"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
